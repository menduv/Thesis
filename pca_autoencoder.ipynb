{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the 50 dimensions is a little too small and I should start with 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your fine-tuned local model \n",
    "model_path = '/home/mendu/Thesis/data/musiccaps/new_embedding_model'\n",
    "model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions = pd.read_csv('/home/mendu/Thesis/data/musiccaps/lp-music-caps_caption_writing.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_dataset = load_dataset('seungheondoh/LP-MusicCaps-MSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(msd_dataset['train'])\n",
    "# test = pd.DataFrame(msd_dataset['test'])\n",
    "# valid = pd.DataFrame(msd_dataset['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train['caption_writing'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2578bff58a174c3eb494d540f071a451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings for your sentences using the fine-tuned model\n",
    "embedded_sentences = model.encode(sentences, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Number of PCA components (e.g., reduce to 50 dimensions)\n",
    "num_components = 50\n",
    "# obj = PCA(n_components=num_components)\n",
    "\n",
    "# Fit the PCA model to the embedded sentences (this will find the principal components)\n",
    "# pca_embeddings = obj.fit_transform(embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Project the PCA embeddings back to the original space\n",
    "# projected_embeddings = obj.inverse_transform(pca_embeddings)\n",
    "\n",
    "# # Initialize an empty list to store the Pearson correlation coefficients\n",
    "# pearsons_correlations = []\n",
    "\n",
    "# # Calculate Pearson's correlation for each pair of original and projected embeddings\n",
    "# for original, projected in zip(embedded_sentences, projected_embeddings):\n",
    "#     # Compute Pearson's r\n",
    "#     corr, _ = pearsonr(original, projected)\n",
    "#     pearsons_correlations.append(corr)\n",
    "\n",
    "# # If you want to compute a single Pearson's correlation coefficient for all data\n",
    "# # Concatenate all embeddings and compute the correlation\n",
    "# flat_original = embedded_sentences.flatten()\n",
    "# flat_projected = projected_embeddings.flatten()\n",
    "# overall_corr, _ = pearsonr(flat_original, flat_projected)\n",
    "\n",
    "# # print(\"Pearsons correlation for each embedding pair:\", pearsons_correlations)\n",
    "# print(\"Overall Pearson's correlation:\", overall_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Original embeddings shape:\", embedded_sentences.shape)\n",
    "# print(\"PCA-reduced embeddings shape:\", pca_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# class Autoencoder(nn.Module):\n",
    "#     def __init__(self, input_size, encoding_size):\n",
    "#         super(Autoencoder, self).__init__()\n",
    "#         # Encoder\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(input_size, encoding_size),\n",
    "#             nn.ReLU(True)\n",
    "#         )\n",
    "#         # Decoder\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(encoding_size, input_size),\n",
    "#             nn.Sigmoid()  # Using Sigmoid because embeddings are likely normalized\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         encoded = self.encoder(x)\n",
    "#         decoded = self.decoder(encoded)\n",
    "#         return decoded\n",
    "    \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(256),  # Added batch normalization\n",
    "            nn.Linear(256, 128),  # Added another layer\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoding_size),  # Adjusted the size of the encoding layer\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_size, 128),  # Adjusted the size of the decoding layer\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),  # Added another layer\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(256),  # Added batch normalization\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.Sigmoid()  # Using Sigmoid because embeddings are likely normalized\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444865, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array of embeddings to a PyTorch tensor\n",
    "embedded_sentences_tensor = torch.tensor(embedded_sentences, dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and a dataloader\n",
    "dataset = TensorDataset(embedded_sentences_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "# # Split dataset into training and validation sets\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# train_loader = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "# val_loader = DataLoader(val_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the NumPy array of embeddings to a PyTorch tensor\n",
    "# embedded_sentences_tensor = torch.tensor(embedded_sentences, dtype=torch.float32)\n",
    "\n",
    "# # Create a dataset and a dataloader without labels\n",
    "# dataset = TensorDataset(embedded_sentences_tensor)\n",
    "\n",
    "# # Split dataset into training and validation sets\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# # Create dataloaders without labels\n",
    "# train_loader = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "# val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset))  # Number of samples in the dataset\n",
    "# print(dataset[0][0].shape)  # Shape of the first sample in the dataset\n",
    "# print(dataset[0][0].ndim)\n",
    "# print(embedded_sentences_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over the batches\n",
    "# for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#     # Print the shape of the data in the first batch\n",
    "#     print(f\"Batch {batch_idx}: Data shape: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the autoencoder\n",
    "input_size = embedded_sentences.shape[1]\n",
    "encoding_size = 64  # change this to whatever size you want to encode down to\n",
    "autoencoder = Autoencoder(input_size=input_size, encoding_size=encoding_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.L1Loss()  # MAE\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.3479156792163849\n",
      "Epoch 2/20, Loss: 0.3374408185482025\n",
      "Epoch 3/20, Loss: 0.33837732672691345\n",
      "Epoch 4/20, Loss: 0.337697833776474\n",
      "Epoch 5/20, Loss: 0.337202787399292\n",
      "Epoch 6/20, Loss: 0.33835944533348083\n",
      "Epoch 7/20, Loss: 0.33562734723091125\n",
      "Epoch 8/20, Loss: 0.3408569097518921\n",
      "Epoch 9/20, Loss: 0.33289599418640137\n",
      "Epoch 10/20, Loss: 0.332951158285141\n",
      "Epoch 11/20, Loss: 0.3391749858856201\n",
      "Epoch 12/20, Loss: 0.3392314910888672\n",
      "Epoch 13/20, Loss: 0.33417224884033203\n",
      "Epoch 14/20, Loss: 0.342439740896225\n",
      "Epoch 15/20, Loss: 0.3379240036010742\n",
      "Epoch 16/20, Loss: 0.33648166060447693\n",
      "Epoch 17/20, Loss: 0.33651337027549744\n",
      "Epoch 18/20, Loss: 0.3340362310409546\n",
      "Epoch 19/20, Loss: 0.3365885317325592\n",
      "Epoch 20/20, Loss: 0.3371446132659912\n"
     ]
    }
   ],
   "source": [
    "epochs = 20  # Set this to the number of epochs to train for\n",
    "for epoch in range(epochs):\n",
    "    for data in dataloader:\n",
    "        inputs = data[0]\n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the EarlyStopping class\n",
    "# class EarlyStopping:\n",
    "#     def __init__(self, patience=5, verbose=False):\n",
    "#         self.patience = patience\n",
    "#         self.verbose = verbose\n",
    "#         self.counter = 0\n",
    "#         self.best_loss = float('inf')\n",
    "#         self.early_stop = False\n",
    "\n",
    "#     def __call__(self, val_loss, model):\n",
    "#         if val_loss < self.best_loss:\n",
    "#             self.best_loss = val_loss\n",
    "#             self.counter = 0\n",
    "#         else:\n",
    "#             self.counter += 1\n",
    "#             if self.counter >= self.patience:\n",
    "#                 self.early_stop = True\n",
    "#                 if self.verbose:\n",
    "#                     print(\"Early stopping\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the EarlyStopping object\n",
    "# early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "# epochs = 50\n",
    "# # Training loop with early stopping\n",
    "# for epoch in range(epochs):\n",
    "#     # Train the model\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     for batch_idx, data in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         # Unpack the batch\n",
    "#         embeddings = data[0]  # Assuming the embeddings are in the first element of the tuple\n",
    "#         output = model(embeddings)\n",
    "#         loss = criterion(output, embeddings)  # Use embeddings as target since it's an autoencoder\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     # Validate the model\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_loader:\n",
    "#             embeddings = data[0]  # Assuming the embeddings are in the first element of the tuple\n",
    "#             output = model(embeddings)\n",
    "#             val_loss += criterion(output, embeddings).item()\n",
    "\n",
    "#     val_loss /= len(val_loader)\n",
    "\n",
    "#     print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss}')\n",
    "\n",
    "#     # Call early stopping\n",
    "#     early_stopping(val_loss, model)\n",
    "\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This constant loss means that we should try going smaller in the embedding space, go to 64 or 32. \n",
    "- DO early .\n",
    "- Higher learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pearson correlation: 0.7092163535337833\n"
     ]
    }
   ],
   "source": [
    "# Switch the autoencoder to evaluation mode\n",
    "autoencoder.eval()\n",
    "\n",
    "# Process the entire dataset to obtain the decoded (projected) embeddings\n",
    "encoded_embeddings = autoencoder.encoder(embedded_sentences_tensor).detach().numpy()\n",
    "decoded_embeddings = autoencoder.decoder(torch.from_numpy(encoded_embeddings)).detach().numpy()\n",
    "\n",
    "# Calculate Pearson's correlation\n",
    "from scipy.stats import pearsonr\n",
    "correlations = np.array([pearsonr(original, decoded)[0] for original, decoded in zip(embedded_sentences, decoded_embeddings)])\n",
    "\n",
    "# print('Pearson correlation for each embedding pair:', correlations)\n",
    "print('Mean Pearson correlation:', np.mean(correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/mendu/Thesis/data/musiccaps/auto_encoder/encoded_embeddings.npy', encoded_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the metric, reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original embeddings shape: torch.Size([444865, 768])\n",
      "Entire dataset encoded embeddings shape: torch.Size([444865, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Original embeddings shape:\", embedded_sentences_tensor.shape)\n",
    "\n",
    "# Switch autoencoder to evaluation mode\n",
    "autoencoder.eval()\n",
    "\n",
    "# Process the entire dataset to obtain the encoded embeddings\n",
    "encoded_embeddings = autoencoder.encoder(embedded_sentences_tensor).detach()\n",
    "\n",
    "print(\"Entire dataset encoded embeddings shape:\", encoded_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to get the word embeddings of the 8 class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer_config.json',\n",
       " 'model.safetensors',\n",
       " 'tokenizer.json',\n",
       " 'vocab.json',\n",
       " '1_Pooling',\n",
       " 'merges.txt',\n",
       " 'sentence_bert_config.json',\n",
       " 'config_sentence_transformers.json',\n",
       " 'modules.json',\n",
       " 'special_tokens_map.json',\n",
       " 'README.md',\n",
       " 'config.json']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('/home/mendu/Thesis/data/musiccaps/new_embedding_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding for the word 'music' is: [  0.       234.11252   55.04939   83.73867  119.17586   24.50513\n",
      "  79.37317    0.         0.       107.77891   85.39018   32.598988\n",
      " 116.71515    0.         0.       177.07715   55.900036  50.565147\n",
      "  70.38419  109.61758    0.       103.22257  105.240326  67.54228\n",
      "  79.07263    0.        78.31037    0.        46.628445 139.69655\n",
      "  41.480885 139.17657  179.46729  204.76526  144.35342  128.00192\n",
      "  36.790775  74.43881    0.       107.29045   92.97456   16.40067\n",
      "  58.436283   0.       112.11439   86.0272     0.         0.\n",
      "  75.80277   79.70243    0.         0.        76.836876  98.40882\n",
      "  91.30781   37.967175  95.21652    0.        40.375088   0.\n",
      "  74.799095 110.38203    0.         0.      ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/mendu/Thesis/data/musiccaps/new_embedding_model/')  # Update the path accordingly\n",
    "\n",
    "# Load the 64-dimensional word embeddings\n",
    "embeddings_reduced = np.load('/home/mendu/Thesis/data/musiccaps/auto_encoder/encoded_embeddings.npy')  # Update with the correct .npy file path\n",
    "\n",
    "# Function to get the embedding of a specific word\n",
    "def get_word_embedding(word):\n",
    "    # Tokenize the word to get its ID\n",
    "    token_id = tokenizer.encode(word, add_special_tokens=False)\n",
    "    if not token_id:\n",
    "        raise ValueError(f\"The word '{word}' was not found in the tokenizer's vocabulary.\")\n",
    "    elif len(token_id) > 1:\n",
    "        raise ValueError(f\"The input text '{word}' corresponds to multiple tokens.\")\n",
    "    token_id = token_id[0]  # We only expect one token ID for a single word input\n",
    "    \n",
    "    # Fetch the corresponding embedding\n",
    "    word_embedding = embeddings_reduced[token_id]\n",
    "    return word_embedding\n",
    "\n",
    "# Example usage:\n",
    "word = \"rock\"  # Replace with the word you're interested in\n",
    "embedding_of_music = get_word_embedding(word)\n",
    "print(f\"The embedding for the word '{word}' is:\", embedding_of_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding for the word rock is: [  0.        126.707634   56.95655    30.001554   90.95441    67.627846\n",
      " 127.68996     0.          0.         59.838825  157.24373    65.84189\n",
      "  49.76614     0.          0.        152.53204    44.524162  109.71577\n",
      "  75.87621   170.42998     0.         51.2337    148.76892   102.406395\n",
      " 132.56421     0.         56.189133    0.         61.006737  139.45685\n",
      "  88.97968    88.43195    81.91134   189.14525   267.908      75.82866\n",
      "  22.694725   74.57384     0.        153.54211    56.44078     2.5905855\n",
      "  98.41678     0.         98.76408    60.717625    0.          0.\n",
      "  65.88503    38.88233     0.          0.         89.83166    64.59985\n",
      " 144.08324    42.727947  107.228676    0.         61.92044     0.\n",
      " 125.440765   74.22465     0.          0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "embedding_of_rock = get_word_embedding(\"rock\")\n",
    "print(f\"The embedding for the word rock is:\", embedding_of_rock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input text 'new age' corresponds to multiple tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_word_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew age\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m, in \u001b[0;36mget_word_embedding\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe word \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m was not found in the tokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms vocabulary.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token_id) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input text \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m corresponds to multiple tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m token_id \u001b[38;5;241m=\u001b[39m token_id[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# We only expect one token ID for a single word input\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Fetch the corresponding embedding\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The input text 'new age' corresponds to multiple tokens."
     ]
    }
   ],
   "source": [
    "get_word_embedding(\"new age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your fine-tuned local model \n",
    "model_path = '/home/mendu/Thesis/data/musiccaps/new_embedding_model'\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Load the tokenizer corresponding to the pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "# Access the vocabulary of the tokenizer\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Assuming you already have the fine-tuned embeddings\n",
    "# For demonstration purposes, let's generate random embeddings\n",
    "fine_tuned_embeddings = np.random.rand(len(vocab), model.get_sentence_embedding_dimension())\n",
    "\n",
    "# Create metadata mapping embeddings to words\n",
    "metadata = {word: embedding.tolist() for word, embedding in zip(vocab.keys(), fine_tuned_embeddings)}\n",
    "\n",
    "# Save the embeddings and metadata\n",
    "np.save('/home/mendu/Thesis/data/musiccaps/auto_encoder/fine_tuned_embeddings.npy', fine_tuned_embeddings)\n",
    "with open('metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_embeddings = np.load('/home/mendu/Thesis/data/musiccaps/auto_encoder/encoded_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nepochs = 50  # Set this to the number of epochs to train for\\nfor epoch in range(epochs):\\n    for data in dataloader:\\n        inputs = data[0]\\n        # Forward pass\\n        outputs = autoencoder(inputs)\\n        loss = criterion(outputs, inputs)\\n\\n        # Backward pass\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        \\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "epochs = 50  # Set this to the number of epochs to train for\n",
    "for epoch in range(epochs):\n",
    "    for data in dataloader:\n",
    "        inputs = data[0]\n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
