{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your fine-tuned local model \n",
    "model_path = '/home/mendu/Thesis/data/musiccaps/new_embedding_model2'\n",
    "model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MusicCaps dataset from HuggingFace\n",
    "msd_dataset = load_dataset('seungheondoh/LP-MusicCaps-MSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using the trainingn set\n",
    "train = pd.DataFrame(msd_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the captions to a list of size 44865\n",
    "sentences = train['caption_writing'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444865"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88264e4e6eac452a83bd33e7707aede2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''By calling model.encode() we are converting the list of entences into encoded vectors'''\n",
    "\n",
    "# Generate embeddings for your sentences using the fine-tuned model\n",
    "embedded_sentences = model.encode(sentences, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Number of PCA components (e.g., reduce to 50 dimensions)\n",
    "# num_components = 50\n",
    "# obj = PCA(n_components=num_components)\n",
    "\n",
    "# Fit the PCA model to the embedded sentences (this will find the principal components)\n",
    "# pca_embeddings = obj.fit_transform(embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444865, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentences.shape #these are our roberta encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Project the PCA embeddings back to the original space\n",
    "# projected_embeddings = obj.inverse_transform(pca_embeddings)\n",
    "\n",
    "# # Initialize an empty list to store the Pearson correlation coefficients\n",
    "# pearsons_correlations = []\n",
    "\n",
    "# # Calculate Pearson's correlation for each pair of original and projected embeddings\n",
    "# for original, projected in zip(embedded_sentences, projected_embeddings):\n",
    "#     # Compute Pearson's r\n",
    "#     corr, _ = pearsonr(original, projected)\n",
    "#     pearsons_correlations.append(corr)\n",
    "\n",
    "# # If you want to compute a single Pearson's correlation coefficient for all data\n",
    "# # Concatenate all embeddings and compute the correlation\n",
    "# flat_original = embedded_sentences.flatten()\n",
    "# flat_projected = projected_embeddings.flatten()\n",
    "# overall_corr, _ = pearsonr(flat_original, flat_projected)\n",
    "\n",
    "# # print(\"Pearsons correlation for each embedding pair:\", pearsons_correlations)\n",
    "# print(\"Overall Pearson's correlation:\", overall_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Original embeddings shape:\", embedded_sentences.shape)\n",
    "# print(\"PCA-reduced embeddings shape:\", pca_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the autoencoder class and architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(256),  # Added batch normalization\n",
    "            nn.Linear(256, 128),  # Added another layer\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, encoding_size),  # Adjusted the size of the encoding layer\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_size, 128),  # Adjusted the size of the decoding layer\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 256),  # Added another layer\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(256),  # Added batch normalization\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.Sigmoid()  # Using Sigmoid because embeddings are likely normalized\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate x_min and x_max from the input\n",
    "        x_min = x.min(dim=0, keepdim=True)[0]\n",
    "        x_max = x.max(dim=0, keepdim=True)[0]\n",
    "        \n",
    "        # Scale input to [0, 1]\n",
    "        x_scaled = (x - x_min) / (x_max - x_min)\n",
    "        \n",
    "        # Encode and decode\n",
    "        encoded = self.encoder(x_scaled)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        # Rescale output to original range\n",
    "        decoded_rescaled = decoded * (x_max - x_min) + x_min\n",
    "        \n",
    "        return decoded_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array of embeddings to a PyTorch tensor\n",
    "embedded_sentences_tensor = torch.tensor(embedded_sentences, dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and a dataloader\n",
    "dataset = TensorDataset(embedded_sentences_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the autoencoder\n",
    "input_size = embedded_sentences.shape[1]\n",
    "encoding_size = 64  # change this to whatever size you want to encode down to\n",
    "autoencoder = Autoencoder(input_size=input_size, encoding_size=encoding_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.L1Loss()  # MAE\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.train_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, train_loss, model):\n",
    "        score = -train_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(train_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(train_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, train_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Training loss decreased ({self.train_loss_min:.6f} --> {train_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), '/home/mendu/Thesis/data/musiccaps/auto_encoder/saved_checkpoints64/checkpoint.pt')\n",
    "        self.train_loss_min = train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.234698\n",
      "Training loss decreased (inf --> 0.234698).  Saving model ...\n",
      "Epoch 2, Training Loss: 0.215427\n",
      "Training loss decreased (0.234698 --> 0.215427).  Saving model ...\n",
      "Epoch 3, Training Loss: 0.211557\n",
      "Training loss decreased (0.215427 --> 0.211557).  Saving model ...\n",
      "Epoch 4, Training Loss: 0.209349\n",
      "Training loss decreased (0.211557 --> 0.209349).  Saving model ...\n",
      "Epoch 5, Training Loss: 0.207929\n",
      "Training loss decreased (0.209349 --> 0.207929).  Saving model ...\n",
      "Epoch 6, Training Loss: 0.206884\n",
      "Training loss decreased (0.207929 --> 0.206884).  Saving model ...\n",
      "Epoch 7, Training Loss: 0.206174\n",
      "Training loss decreased (0.206884 --> 0.206174).  Saving model ...\n",
      "Epoch 8, Training Loss: 0.205653\n",
      "Training loss decreased (0.206174 --> 0.205653).  Saving model ...\n",
      "Epoch 9, Training Loss: 0.205247\n",
      "Training loss decreased (0.205653 --> 0.205247).  Saving model ...\n",
      "Epoch 10, Training Loss: 0.204743\n",
      "Training loss decreased (0.205247 --> 0.204743).  Saving model ...\n",
      "Epoch 11, Training Loss: 0.204370\n",
      "Training loss decreased (0.204743 --> 0.204370).  Saving model ...\n",
      "Epoch 12, Training Loss: 0.204117\n",
      "Training loss decreased (0.204370 --> 0.204117).  Saving model ...\n",
      "Epoch 13, Training Loss: 0.203793\n",
      "Training loss decreased (0.204117 --> 0.203793).  Saving model ...\n",
      "Epoch 14, Training Loss: 0.203565\n",
      "Training loss decreased (0.203793 --> 0.203565).  Saving model ...\n",
      "Epoch 15, Training Loss: 0.203272\n",
      "Training loss decreased (0.203565 --> 0.203272).  Saving model ...\n",
      "Epoch 16, Training Loss: 0.203122\n",
      "Training loss decreased (0.203272 --> 0.203122).  Saving model ...\n",
      "Epoch 17, Training Loss: 0.202833\n",
      "Training loss decreased (0.203122 --> 0.202833).  Saving model ...\n",
      "Epoch 18, Training Loss: 0.202742\n",
      "Training loss decreased (0.202833 --> 0.202742).  Saving model ...\n",
      "Epoch 19, Training Loss: 0.202554\n",
      "Training loss decreased (0.202742 --> 0.202554).  Saving model ...\n",
      "Epoch 20, Training Loss: 0.202360\n",
      "Training loss decreased (0.202554 --> 0.202360).  Saving model ...\n",
      "Epoch 21, Training Loss: 0.202173\n",
      "Training loss decreased (0.202360 --> 0.202173).  Saving model ...\n",
      "Epoch 22, Training Loss: 0.201878\n",
      "Training loss decreased (0.202173 --> 0.201878).  Saving model ...\n",
      "Epoch 23, Training Loss: 0.201758\n",
      "Training loss decreased (0.201878 --> 0.201758).  Saving model ...\n",
      "Epoch 24, Training Loss: 0.201552\n",
      "Training loss decreased (0.201758 --> 0.201552).  Saving model ...\n",
      "Epoch 25, Training Loss: 0.201315\n",
      "Training loss decreased (0.201552 --> 0.201315).  Saving model ...\n",
      "Epoch 26, Training Loss: 0.201170\n",
      "Training loss decreased (0.201315 --> 0.201170).  Saving model ...\n",
      "Epoch 27, Training Loss: 0.200921\n",
      "Training loss decreased (0.201170 --> 0.200921).  Saving model ...\n",
      "Epoch 28, Training Loss: 0.200683\n",
      "Training loss decreased (0.200921 --> 0.200683).  Saving model ...\n",
      "Epoch 29, Training Loss: 0.200564\n",
      "Training loss decreased (0.200683 --> 0.200564).  Saving model ...\n",
      "Epoch 30, Training Loss: 0.200263\n",
      "Training loss decreased (0.200564 --> 0.200263).  Saving model ...\n",
      "Epoch 31, Training Loss: 0.200100\n",
      "Training loss decreased (0.200263 --> 0.200100).  Saving model ...\n",
      "Epoch 32, Training Loss: 0.199944\n",
      "Training loss decreased (0.200100 --> 0.199944).  Saving model ...\n",
      "Epoch 33, Training Loss: 0.199710\n",
      "Training loss decreased (0.199944 --> 0.199710).  Saving model ...\n",
      "Epoch 34, Training Loss: 0.199531\n",
      "Training loss decreased (0.199710 --> 0.199531).  Saving model ...\n",
      "Epoch 35, Training Loss: 0.199235\n",
      "Training loss decreased (0.199531 --> 0.199235).  Saving model ...\n",
      "Epoch 36, Training Loss: 0.199119\n",
      "Training loss decreased (0.199235 --> 0.199119).  Saving model ...\n",
      "Epoch 37, Training Loss: 0.198820\n",
      "Training loss decreased (0.199119 --> 0.198820).  Saving model ...\n",
      "Epoch 38, Training Loss: 0.198717\n",
      "Training loss decreased (0.198820 --> 0.198717).  Saving model ...\n",
      "Epoch 39, Training Loss: 0.198452\n",
      "Training loss decreased (0.198717 --> 0.198452).  Saving model ...\n",
      "Epoch 40, Training Loss: 0.198220\n",
      "Training loss decreased (0.198452 --> 0.198220).  Saving model ...\n",
      "Epoch 41, Training Loss: 0.197999\n",
      "Training loss decreased (0.198220 --> 0.197999).  Saving model ...\n",
      "Epoch 42, Training Loss: 0.197885\n",
      "Training loss decreased (0.197999 --> 0.197885).  Saving model ...\n",
      "Epoch 43, Training Loss: 0.197758\n",
      "Training loss decreased (0.197885 --> 0.197758).  Saving model ...\n",
      "Epoch 44, Training Loss: 0.197573\n",
      "Training loss decreased (0.197758 --> 0.197573).  Saving model ...\n",
      "Epoch 45, Training Loss: 0.197354\n",
      "Training loss decreased (0.197573 --> 0.197354).  Saving model ...\n",
      "Epoch 46, Training Loss: 0.197159\n",
      "Training loss decreased (0.197354 --> 0.197159).  Saving model ...\n",
      "Epoch 47, Training Loss: 0.197067\n",
      "Training loss decreased (0.197159 --> 0.197067).  Saving model ...\n",
      "Epoch 48, Training Loss: 0.196869\n",
      "Training loss decreased (0.197067 --> 0.196869).  Saving model ...\n",
      "Epoch 49, Training Loss: 0.196745\n",
      "Training loss decreased (0.196869 --> 0.196745).  Saving model ...\n",
      "Epoch 50, Training Loss: 0.196503\n",
      "Training loss decreased (0.196745 --> 0.196503).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 50  # Set this to the number of epochs to train for\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    autoencoder.train()\n",
    "    train_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        inputs = data[0]\n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Training Loss: {train_loss:.6f}')\n",
    "\n",
    "    early_stopping(train_loss, autoencoder)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the last checkpoint with the best model\n",
    "autoencoder.load_state_dict(torch.load('/home/mendu/Thesis/data/musiccaps/auto_encoder/saved_checkpoints64/checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output range of the autoencoder: [-15.236154556274414, 20.39266014099121]\n"
     ]
    }
   ],
   "source": [
    "# Check the range of the output\n",
    "with torch.no_grad():\n",
    "    sample_output = autoencoder(embedded_sentences_tensor)\n",
    "    output_min = sample_output.min().item()\n",
    "    output_max = sample_output.max().item()\n",
    "\n",
    "print(f\"Output range of the autoencoder: [{output_min}, {output_max}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoder's state_dict\n",
    "torch.save(autoencoder.encoder.state_dict(), '/home/mendu/Thesis/data/musiccaps/auto_encoder/encoder_state_dict64.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Switch the autoencoder to evaluation mode\n",
    "# autoencoder.eval()\n",
    "\n",
    "# # Process the entire dataset to obtain the decoded (projected) embeddings\n",
    "# encoded_embeddings = autoencoder.encoder(embedded_sentences_tensor).detach().numpy()\n",
    "# decoded_embeddings = autoencoder.decoder(torch.from_numpy(encoded_embeddings)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoder's state_dict\n",
    "# torch.save(autoencoder.encoder.state_dict(), '/home/mendu/Thesis/data/musiccaps/auto_encoder/encoder_state_dict128.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the metric, reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Original embeddings shape:\", embedded_sentences_tensor.shape)\n",
    "\n",
    "# # Switch autoencoder to evaluation mode\n",
    "# autoencoder.eval()\n",
    "\n",
    "# # Process the entire dataset to obtain the encoded embeddings\n",
    "# encoded_embeddings = autoencoder.encoder(embedded_sentences_tensor).detach()\n",
    "\n",
    "# print(\"Entire dataset encoded embeddings shape:\", encoded_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to get the word embeddings of the 8 class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have your SentenceTransformer model loaded as `model`\n",
    "# new_sentence = \"Your new sentence.\"\n",
    "# new_sentence_embedding = model.encode(new_sentence, convert_to_tensor=True)\n",
    "\n",
    "# # Create a new Autoencoder instance and load the trained encoder\n",
    "# autoencoder = Autoencoder(input_size=768, encoding_size=64)\n",
    "# encoder_state_dict = torch.load('/home/mendu/Thesis/data/musiccaps/auto_encoder/encoder_state_dict.pth')\n",
    "# autoencoder.encoder.load_state_dict(encoder_state_dict)\n",
    "\n",
    "# # You may need to ensure the new sentence embedding is on the same device (CPU/GPU) as the model\n",
    "# # e.g., if the autoencoder is on the GPU, you need to do: new_sentence_embedding = new_sentence_embedding.to('cuda')\n",
    "\n",
    "# # Pass your new sentence embedding through the encoder\n",
    "# autoencoder.eval()  # Important: set the model to evaluation mode\n",
    "# with torch.no_grad():\n",
    "#     new_encoded_embedding = autoencoder.encoder(new_sentence_embedding.unsqueeze(0))  # Add dummy batch dimension\n",
    "\n",
    "# # Convert to numpy array if needed\n",
    "# new_encoded_embedding = new_encoded_embedding.cpu().numpy()  # Call .cpu() if model is on GPU\n",
    "\n",
    "# # The variable 'new_encoded_embedding' now contains the 64-dimensional vector for the new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.listdir('/home/mendu/Thesis/data/musiccaps/new_embedding_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Chnage this, dont use the tokeniser use model\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Load the fine-tuned tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/home/mendu/Thesis/data/musiccaps/new_embedding_model/')  # Update the path accordingly\n",
    "\n",
    "# # Load the 64-dimensional word embeddings\n",
    "# embeddings_reduced = np.load('/home/mendu/Thesis/data/musiccaps/auto_encoder/encoded_embeddings.npy')  # Update with the correct .npy file path\n",
    "\n",
    "# # Function to get the embedding of a specific word\n",
    "# def get_word_embedding(word):\n",
    "#     # Tokenize the word to get its ID\n",
    "#     token_id = tokenizer.encode(word, add_special_tokens=False)\n",
    "#     if not token_id:\n",
    "#         raise ValueError(f\"The word '{word}' was not found in the tokenizer's vocabulary.\")\n",
    "#     elif len(token_id) > 1:\n",
    "#         raise ValueError(f\"The input text '{word}' corresponds to multiple tokens.\")\n",
    "#     token_id = token_id[0]  # We only expect one token ID for a single word input\n",
    "    \n",
    "#     # Fetch the corresponding embedding\n",
    "#     word_embedding = embeddings_reduced[token_id]\n",
    "#     return word_embedding\n",
    "\n",
    "# # Example usage:\n",
    "# word = \"rock\"  # Replace with the word you're interested in\n",
    "# embedding_of_music = get_word_embedding(word)\n",
    "# print(f\"The embedding for the word '{word}' is:\", embedding_of_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Older autoencoder code\n",
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# class Autoencoder(nn.Module):\n",
    "#     def __init__(self, input_size, encoding_size):\n",
    "#         super(Autoencoder, self).__init__()\n",
    "#         # Encoder\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(input_size, encoding_size),\n",
    "#             nn.ReLU(True)\n",
    "#         )\n",
    "#         # Decoder\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(encoding_size, input_size),\n",
    "#             nn.Sigmoid()  # Using Sigmoid because embeddings are likely normalized\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         encoded = self.encoder(x)\n",
    "#         decoded = self.decoder(encoded)\n",
    "#         return decoded\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
